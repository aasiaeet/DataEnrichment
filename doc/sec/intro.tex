
\section{Introduction}
Over the past two decades, major advances have been made in estimating structured parameters, e.g., sparse, low-rank, etc., in high-dimensional small sample problems [][][].
Such estimators consider a suitable (semi) parametric model of the response: $y = \phi(\x,\bbeta^*) + \omega$ based on $n$ samples $\{(\x_i,y_i),i=1,\ldots,n\}$ and $\beta \in \reals^p$
is the parameter of interest. The unique aspect of such high-dimensional setup is that the number of samples $n < p$, and the structure in $\bbeta^*$, e.g., sparsity, low-rank, makes
the estimation possible [][][]. While the earlier developments in such high dimensional estimation problems had focused on parametric linear models, the results
have been widely extended to non-linear models, e.g., generalized linear models [][], broad families of semi-parametric and single-index models [][], non-convex models [][], etc.

In several real world problems, the assumption that one global model parameter $\bbeta$ is suitable for the entire population is unrealistic.
We consider the more general setting where the population consists of sub-populations (groups) which are similar is many aspects but are different in certain
unique aspects. For example, in the context of anti-cancer drug sensitivity prediction where the goal is to predict responses of different tumor cells to a drug, using a same prediction model for all cancer types (groups) ignores the specific aspects of each group and leads to an uninterpretable global model. 
%\ab{... a line on drug sensitivity of cancer cell lines ...}. 
%
%Consider the problem of predicting an outcome for a population which consists of sub-populations (groups) that are similar in some aspects but different in others. Focusing the exposition to linear models, 
%
In such a setting, one can assume a model for each group based on a group specific parameter $\bbeta^*_g$, i.e., $y = \phi(\x, \bbeta_g^*) + \omega$ for group $g$. 
%where $g$ and $i$ index the group and samples therein respectively. 
Such a modeling approach fails to leverage the similarities across the sub-populations, and can only be estimated when sufficient number of samples are available 
for each sub-population which is not the case in several problems, e.g., anti-cancer drug sensitivity prediction \cite{barretina2012cancer, iorio2016landscape1}.
%
%Besides, to efficiently estimate per group parameters $\bbeta^*_g$ one needs a suitably large number of samples for all group.
%Alternatively, one can ignore the grouping information and build a global model by lumping together all of the samples from heterogeneous groups and fit a single linear model $y_i = \x_i^T \bbeta^* + \omega_i$. While any prediction procedure based on this assumption benefits from a larger number of samples, it fails to capture unique aspects of each group which may lead to poorer prediction accuracy and less interpretable estimated parameter.

In this paper, we consider the \emph{data enrichment} strategy recently suggested in the literature \cite{domu16, grti16,  olvi14, olvi15} for the above problem setup. 
%\ab{I still like the `data sharing' name -- since it has been used in the literature, it will help a reader connect to the existing body of work.} 
A data enriched model \emph{enriches} above models by taking their middle ground. It assumes that there is a \emph{common} parameter $\bbeta^*_0$ shared between all groups and \emph{individual} per-group parameters $\bbeta_g^*$ which characterize the deviation of group $g$, i.e.,
%\be
%\label{eq:dsl}
%y_{gi} &=& \x_{gi}^T (\bbeta_0^* + \bbeta^*_g) + \omega_{gi}, \quad g \in \{1, \dots, G\}~,
%\ee
\be
\label{eq:dsl}
y_{gi} &=& \phi(\x_{gi}, (\bbeta_0^* + \bbeta^*_g)) + \omega_{gi}, \quad g \in \{1, \dots, G\}~,
\ee
where $g$ and $i$ index the group and samples respectively. In \eqref{eq:dsl}, we have $G$ 
models coupled by the common parameter $\bbeta^*_0$ which models similarities between all samples. Individual parameters $\bbeta_g$s capture unique aspects of each group.
We specifically focus on the high-dimensional small sample regime for \eqref{eq:dsl} where the number of samples $n_g$ for each group is much smaller than the ambient 
dimensionality, i.e., $\forall g: n_g \ll p$. Similar to all other high-dimensional models, we assume that the parameters $\bbeta_g$ are structured, i.e., for suitable convex functions $f_g$s, $f_g(\bbeta_g)$ is small.
For example, when the structure is sparsity, $f_g$s are $L_1$-norms. Further, for the technical analysis and proofs,
we focus on the case of linear models, i.e., $\phi(\x,\bbeta) = \x^T \bbeta$. The results
seamlessly extend to more general non-linear models, e.g., generalized linear models [][], broad families of semi-parametric and single-index models [][], non-convex models [][], etc., using
existing results, i.e., how models like Lasso have been extended to these settings [][]. \ab{Is the last line correct? Lets check this.}

%We call these set of models, \emph{``data sharing''} model \cite{grti16}.
%In high dimensional regime, we assume that both common and individual parameters are structured, i.e., for suitable \emph{convex} functions $f_g$, $f_g(\bbeta^*_g)$s are small.
% and one desirable scenario is when the common parameter is much denser than the individual ones.
%In other words, for $g \in \{1, \dots, G\}$ $\bbeta^*_g$s are $s_g$-sparse vectors where $s_0 \gg s_g$.
%The common parameter $\bbeta^*_0$ captures the shared effect of features among groups.
%the ``dense similarity'' and individual parameters capture ``slight difference'' between groups and hopefully common parameter estimation will benefit from all of the shared data.

Note that the models in \eqref{eq:dsl} for each group is a superposition \cite{guba16, mctr13}, or dirty statistical model \cite{yara13}. %\ab{cite relevant paper(s) by McCoy-Tropp on convex de-mixing} Therefore, a {\em data-enriched}
(DE) model is \emph{a system of coupled superposition models}.
A related model is proposed by \cite{jrsr10} in the context of multi-task learning, where for each task $g$ the output is coming from $y_{gi} = \x_{gi}^T (\bbeta_{0g}^* + \bbeta^*_g) + \omega_{gi}$.
As emphasized by the subscript of $\bbeta_{0g}^*$ the common parameters are different in every task but they share a same support (index of non-zero values), i.e., $\bbeta_{0i}^* \neq \bbeta_{0j}^*$ but $\text{supp}(\bbeta_{0i}^*) = \text{supp}(\bbeta_{0j}^*)$. \ab{need a line on what Jalali et al.~show, and how our results are (qualitatively) different} 
\ab{we don't have a discussion on `related work' -- including hierarchical models, multi-task learning; should we have a sub-section on these, or otherwise discuss these related developments}
%Also \cite{jrsr10} considers only the cases where function $f_g(\cdot)$ are $l_1$-norm.
At a high level, the DE model can be viewed as a
of hierarchical additive model [][], where $\bbeta_0^*$ corresponds to the root and $\bbeta_g^*$ correspond to the refinements in the leaves. 


The DE model where $\bbeta_g$s are sparse has recently gained attention because of its application in wide range of domains such as personalized medicine \cite{domu16}, sentiment analysis, banking strategy \cite{grti16}, single cell data analysis \cite{olvi15}, road safety \cite{olvi14}, and disease subtype analysis \cite{domu16}.
More generally, in any high-dimensional problem where the population consists of groups, data enrichment framework has the potential to boost the prediction accuracy and results in a more interpretable set of parameters.

In spite of the recent surge in applying data enrichment framework to different domains, limited advances have been made in
understanding the statistical and computational properties of suitable estimators for the data enriched model.
In fact, non-asymptotic statistical properties, including sample complexity and statistical rates of convergence, of regularized estimators for the data enriched model is still an open question \cite{grti16, olvi14}.
To the best of our knowledge, the only theoretical guarantee for data enrichment is provided in \cite{olvi15} where authors prove sparsistency of their proposed method under the stringent irrepresentability condition of the design matrix.\ab{If they show support recovery, the condition may have been necessary -- maybe give them due credit, and show how much more the current results are, though we are doing norm consistency, not support recovery.}
Also beyond sparsity and $l_1$-norm, no other structure has been investigated for the data enriched model. Moreover, no computational results, such as rates of convergence of the optimization algorithms
associated with the proposed estimators, exist in the literature.
%{\color{red} TODO: Emphasis that functions can be non-convex.}

%\newlength\heightfiga\newlength\heightcapa
%\newlength\heightfigb\newlength\heightcapb
%\newlength\heightfigc\newlength\heightcapc
%\newlength\heightfig
%
%\begin{figure*}
%	%Definition of lengths
%	\setlength\heightfiga{3cm}
%	\setlength\heightfigb{3cm}
%	\setlength\heightcapa{1\baselineskip}
%	\setlength\heightcapb{2\baselineskip}
%	%Do not change
%	\setlength\heightcapc{\heightcapa}
%	\setlength\heightfigc{\heightfiga+\heightfigb+\heightcapa}
%	\setlength\heightfig{\heightfigc+\heightcapc}
%	\begin{minipage}[b][\heightfig][t]{0.68\linewidth}
%		\centering
%		\includegraphics[height=\heightfigc]{./img/concept}
%		\parbox[b][\heightcapc][t]{1.\linewidth}{\subcaption{Data Enrichment.}\label{subfig3}}
%	\end{minipage}	\hfill
%	\begin{minipage}[b][\heightfig][t]{0.3\linewidth}\centering
%		\includegraphics[height=\heightfiga]{./img/sc.pdf}
%		\parbox[b][\heightcapa][t]{1\linewidth}{\subcaption{SC}\label{subfig1}}
%		\includegraphics[height=\heightfigb]{./img/deric.pdf}
%		\parbox[b][\heightcapb][t]{1\linewidth}{\subcaption{DERIC}\label{subfig2}}
%	\end{minipage}
%	\caption{a) A conceptual illustration of data enrichment model for learning representation of  different cat species. The common parameter $\bbeta_0$ captures a \emph{generic cat} which consists of shared features among all cats. b) Our  c) Distribution of responses to Saracatinib. Note that some cell lines of both lung and blood cancers have responded to Saracatinib which makes it a good candidate for interpretability analysis.}
%	\label{fig}
%\end{figure*}

\begin{figure}
		\centering
		\includegraphics[scale=.39]{./img/concept.pdf}
		\caption{A conceptual illustration of data enrichment model for learning representation of  different cat species. The common parameter $\bbeta_0$ captures a \emph{generic cat} which consists of shared features among all cats.}
		\label{fig:cat}
\end{figure}



%{\bf Contributions:}
{\bf Notation and Preliminaries:}
We denote sets by curly $\cV$, matrices by bold capital $\V$, random variables by capital $V$, and vectors by small bold $\v$ letters.
We take $[G] = \{0, \dots, G\}$ and $[G]_\setminus = [G] \setminus \{0\}$.
%\ab{Can we use $[G]= \{1,\ldots,G\}$ and use $G \cup \{0\}$ in places where we need to include $\{0\}$? The notation will be more explicit and easier to follow.} 

Given $G$ groups and $n_g$ samples in each as $\{ \{\x_{gi}, y_{gi} \}_{i=1}^{n_g} \}_{g = 1}^G$, we can form the per group design matrix $\X_g \in \reals^{n_g \times p}$ and output vector $\y_g \in \reals ^{n_g}$.
The total number of samples is  $n = \sum_{g = 1}^{G} n_g$.
The data enriched model takes the following vector form:
\be
\label{eq:dirtymodel}
\y_g = \X_g (\bbeta _0^* + \bbeta _g^*) + \oomega_g,  \quad \forall g \in [G]_\setminus
\ee
where each row of $\X_g$ is $\x_{gi}^T$ and $\oomega_g^T = (\omega_{g1}, \dots, \omega_{gn_g})$ is the noise vector.
% are indexed with either a single number as $\v(i)$ or an index set $\cA$ as $\v_{\cA}$.
%Row $i$ of the matrix $\V$ is shown as $\v_i$ and $j$th element of the vector $\v$ is shown as $\v(j)$.
%The $(i,j)$th element of the matrix $\V$ is shown in three ways: $\V_{ij}$, $\v_i(j)$, or $v_{ij}$.
%Throughout the manuscript $c_i$ and $C_i$ are positive constants.
%We name the sample covariance of any matrix $M$ by $S_M$ vs. the actual parameter $\Sigma_M$.

%\noindent {\bf Sub-Gaussian (Sub-exponential) random variable and vector.}
A random variable $V$ is sub-Gaussian if its moments satisfies $\forall p \geq 1: (\ex |V|^p )^{1/p} \leq K_2 \sqrt{p}$.
The minimum value of $K_2$ is called the sub-Gaussian  norm of $V$, denoted by $\normth{V}{\psi_2}$ \cite{vers12}.
A random vector $\v \in \reals^p$ is sub-Gaussian if the one-dimensional marginals $\langle \v, \u \rangle$ are sub-Gaussian random variables for all $\u \in \reals^p$. The sub-Gaussian norm of $\v$ is defined \cite{vers12} as $\normth{\v}{\psi_2} = \sup_{\u \in \sphere} \normth{\langle \v, \u \rangle}{\psi_2}$.
%We abuse notation and use shorthand $\v \sim \subg(0, \Sigma_{\v}, K_{\v})$ for zero mean sub-Gaussian random vector with covariance $\Sigma_{\v}$ and sub-Gaussian norm of $K_{\v}$, although keeping in mind that no other moments, nor the exact form of the distribution function is known.
For any set $\cV \in \reals^p$ the Gaussian width of the set $\cV$ is defined as $\omega(\cV) = \ex_\g \left[ \sup_{\u \in \cV} \langle \g, \u \rangle \right]$ \cite{vershynin2018high}, where the expectation is over $\g \sim N(\0, \I_{p \times p})$, a vector of independent zero-mean unit-variance Gaussian.

%We define the minimum and maximum eigenvalues of a matrix $\M$ restricted to the set $\cA \subseteq \eS^{p-1}$ as $\lambda_{\min}(\M|\cA) = \inf_{\u \in \cA} \u^T \M \u$, and $\lambda_{\max}(\M|\cA) = \sup_{\u \in \cA} \u^T \M \u$ respectively.
%All $c_i$, $c$, and $C$ represent universal constants throughout the manuscript.
%Set $[G] = \{0, \dots, G\}$ is the index set for both shared and individual components (in the setting of data enriched model \eqref{eq:dsl}) and $[G]_\setminus = [G] - \{ 0 \}$ represents only the individual ones.


%\subsection{Contributions}
{\bf Contributions:}
We propose the following Data Enrichment (DE) estimator $\hbbe$ for recovering the structured parameters where the structure is induced by \emph{convex} functions $f_g(\cdot)$:
{\small\be
	\label{eq:super}
	\hbbe = (\hbbe_0^T, \dots, \hbbe_G^T) \in \argmin_{\bbeta _0, \dots, \bbeta _G} \frac{1}{n} \sum_{g=1}^{G} \norm{\y_g - \X_g (\bbeta _0 + \bbeta _g)}{2}^2,
	\\ \nr
	\text{s.t.} \quad \forall g \in [G]:f_g(\bbeta _g) \leq f_g(\bbeta _g^*).
\ee}

%We are investigating the conjecture that the pooled data from all groups will facilitate estimation of the common parameter $\bbeta_0^*$ in both samples complexity and error-bound rate regards.
%In our work, we explicitly answer these questions as follows:

We present several statistical and computational results for the DE estimator \eqref{eq:super} of the data enriched model:
\begin{itemize}[leftmargin = .4cm]
	\item The DE estimator \eqref{eq:super} succeeds if a geometric condition that we call \emph{Data EnRichment Incoherence Condition} (DERIC) is satisfied. Compared to other known geometric conditions in the literature such as structural coherence \cite{guba16} and stable recovery conditions \cite{mctr13}, DERIC is a considerably weaker condition.
	\item Assuming DERIC holds, we establish a high probability non-asymptotic bound on the weighted sum of parameter-wise estimation error, $\ddelta_g = \hbbe_g - \bbeta_g^*$ as:
	{\small \be
	\label{eq:errorsum}
	\sum_{g=0}^{G}  \sqrt{\frac{n_g}{n}} \|\ddelta_g\|_2 \leq  C \gamma \frac{\max_{g \in [G]} \omega(\cC_g \cap \sphere) + \sqrt{\log (G+1)}}{\sqrt{n}},
	\ee}
	where $n_g$ is number of samples per group, $n_0 \triangleq n$ is the total number of samples, $\gamma \triangleq \max_{g \in [G] } \frac{n}{n_g}$ is the \emph{sample condition number}, and $\cC_g$ is the error cone corresponding to $\bbeta_g^*$ exactly defined in Section \ref{sec:deter}.
	To the best of our knowledge, this is the first statistical estimation guarantee for the data enriched model.
	%Gaussian width of a set $\cS$ is $\omega(\cS) = \ex_\g \left[ \sup_{\u \in \cS} \langle \g, \u \rangle \right]$. Gaussian width has been a standard tool for capturing the complexity of high-dimensional problems \cite{venkat12}.
	
%	Similar to previous works, we make a geometric assumption regarding the relation $\cC_g$s in each individual problem which is known as Structural Coherence assumption \cite{guba16, trop15}.
%	\item The general bound of \eqref{eq:errorsum} entails the following bounds for specific parameters:
%	\be
%	\nr
%	\forall g \in [G]: \norm{\ddelta_g}{2} \leq c \gamma \frac{\max_{g \in [G]} \omega(\cC_g \cap \sphere) + c\sqrt{\log G}}{\sqrt{n_g}}
%	\ee
%	Observe that $l_2$-norm of the estimation error for the common parameter decays as $1/\sqrt{n}$, similar to the well-studied high dimensional regression case \cite{venkat12, banerjee14}.
%	So the common parameter's estimator exploits all of the pooled data to reduce its error.
	\item We also establish the sample complexity of the DE estimator for all parameters as $\forall g \in [G]: n_g = O(\omega(\cC_g \cap \sphere))^2$. We emphasize that our result proofs that the recovery of the common parameter $\bbeta_0$ by DE estimator benefits from \emph{all} of the $n$ pooled samples.
	\item We present an efficient Projected Block Gradient Descent (PBGD) algorithm to solve DE's objective \eqref{eq:super} which converges geometrically to the statistical error bound of \eqref{eq:errorsum}. To the best of our knowledge, this is the first rigorous computational result for the high-dimensional data-enriched regression.
	\item We illustrate promising empirical performance of the model on synthetic data as well as on the problem of finding bio-markers associated with drug sensitivity of cell lines from different cancer types, where the support of individual parameters $\hbbe_g^T$ for each cancer $g$ shows a different set of bio-markers per cancer type.
\end{itemize}

The rest of this paper is organized as follows:
%First, we present a review of the related works in Section \ref{relwork}.
First, we characterize the error set of our estimator and provide a deterministic error bound in Section \ref{sec:esti}.
Then in Section \ref{sec:re}, we discuss the restricted eigenvalue condition and calculate the sample complexity required for the recovery of the true parameters by our estimator under DERIC condition.
We close the statistical analysis in Section \ref{sec:error} by providing non-asymptotic high probability error bound for parameter recovery.
We delineate our linearly convergent algorithm, PBGD in Section \ref{sec:opt} and finally supplement our work with synthetic and real data experiments in Sections \ref{sec:expds} and \ref{realexp}.

\begin{figure}[t!]
	\centering
	\begin{subfigure}[t]{0.23\textwidth}
		\includegraphics[width=\textwidth]{./img/sc.pdf}
		\caption{Structural Coherence (SC) condition.}\label{fig:sc}
	\end{subfigure} 
	~
	\begin{subfigure}[t]{0.23\textwidth}
		\includegraphics[width=1.04\textwidth]{./img/deric.pdf}
		\caption{Data EnRichment Incoherence Condition (DERIC). }
		\label{fig:deric}
	\end{subfigure}
	\caption{a) State of the art condition for recovering common and individual parameters in superposition models where $\cC_g = \text{Cone}(\cE_g)$ are error cones and $\cE_g = \left\{\ddelta_g | f_g(\bbeta _g^* + \ddelta_g) \leq f_g(\bbeta _g^*)\right\}$ are the error sets for each parameter $\bbeta^*_g \in [G]$ \cite{guba16} b) Our more relaxed recovery condition which allows \emph{arbitrary non-zero fraction } of the error cones of individual parameters intersect with $-\cC_0$.}
	\label{fig syn2}
\end{figure}

