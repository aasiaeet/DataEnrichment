% Generated by IEEEtran.bst, version: 1.12 (2007/01/11)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{candes2010power}
E.~J. Cand{\`e}s and T.~Tao, ``The power of convex relaxation: Near-optimal
  matrix completion,'' \emph{IEEE Transactions on Information Theory}, vol.~56,
  no.~5, pp. 2053--2080, 2010.

\bibitem{donoho2006compressed}
D.~L. Donoho, ``Compressed sensing,'' \emph{IEEE Transactions on information
  theory}, vol.~52, no.~4, pp. 1289--1306, 2006.

\bibitem{friedman2008sparse}
J.~Friedman, T.~Hastie, and R.~Tibshirani, ``Sparse inverse covariance
  estimation with the graphical lasso,'' \emph{Biostatistics}, vol.~9, no.~3,
  pp. 432--441, 2008.

\bibitem{candes2009exact}
E.~J. Cand{\`e}s and B.~Recht, ``Exact matrix completion via convex
  optimization,'' \emph{Foundations of Computational mathematics}, vol.~9,
  no.~6, p. 717, 2009.

\bibitem{candes2006robust}
E.~J. Cand{\`e}s, J.~Romberg, and T.~Tao, ``Robust uncertainty principles:
  Exact signal reconstruction from highly incomplete frequency information,''
  \emph{IEEE Transactions on information theory}, vol.~52, no.~2, pp. 489--509,
  2006.

\bibitem{tibshirani1996regression}
R.~Tibshirani, ``Regression shrinkage and selection via the lasso,''
  \emph{Journal of the Royal Statistical Society. Series B (Methodological)},
  pp. 267--288, 1996.

\bibitem{bach2012optimization}
F.~Bach, R.~Jenatton, J.~Mairal, G.~Obozinski \emph{et~al.}, ``Optimization
  with sparsity-inducing penalties,'' \emph{Foundations and
  Trends{\textregistered} in Machine Learning}, vol.~4, no.~1, pp. 1--106,
  2012.

\bibitem{negahban2009unified}
S.~Negahban, B.~Yu, M.~J. Wainwright, and P.~K. Ravikumar, ``A unified
  framework for high-dimensional analysis of $ m $-estimators with decomposable
  regularizers,'' in \emph{Advances in Neural Information Processing Systems},
  2009, pp. 1348--1356.

\bibitem{boufounos20081}
P.~T. Boufounos and R.~G. Baraniuk, ``1-bit compressive sensing,'' in
  \emph{Information Sciences and Systems, 2008. CISS 2008. 42nd Annual
  Conference on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2008, pp.
  16--21.

\bibitem{plan2017high}
Y.~Plan, R.~Vershynin, and E.~Yudovina, ``High-dimensional estimation with
  geometric constraints,'' \emph{Information and Inference: A Journal of the
  IMA}, vol.~6, no.~1, pp. 1--40, 2017.

\bibitem{blumensath2009iterative}
T.~Blumensath and M.~E. Davies, ``Iterative hard thresholding for compressed
  sensing,'' \emph{Applied and computational harmonic analysis}, vol.~27,
  no.~3, pp. 265--274, 2009.

\bibitem{jain2013low}
P.~Jain, P.~Netrapalli, and S.~Sanghavi, ``Low-rank matrix completion using
  alternating minimization,'' in \emph{Proceedings of the forty-fifth annual
  ACM symposium on Theory of computing}.\hskip 1em plus 0.5em minus 0.4em\relax
  ACM, 2013, pp. 665--674.

\bibitem{barretina2012cancer}
J.~Barretina, G.~Caponigro, N.~Stransky, K.~Venkatesan, A.~A. Margolin, S.~Kim,
  C.~J. Wilson, J.~Leh{\'a}r, G.~V. Kryukov, D.~Sonkin \emph{et~al.}, ``The
  cancer cell line encyclopedia enables predictive modelling of anticancer drug
  sensitivity,'' \emph{Nature}, vol. 483, no. 7391, p. 603, 2012.

\bibitem{iorio2016landscape1}
F.~Iorio, T.~A. Knijnenburg, D.~J. Vis, G.~R. Bignell, M.~P. Menden,
  M.~Schubert, N.~Aben, E.~Goncalves, S.~Barthorpe, H.~Lightfoot \emph{et~al.},
  ``A landscape of pharmacogenomic interactions in cancer,'' \emph{Cell}, vol.
  166, no.~3, pp. 740--754, 2016.

\bibitem{guba16}
Q.~Gu and A.~Banerjee, ``High dimensional structured superposition models,'' in
  \emph{Advances In Neural Information Processing Systems}, 2016, pp.
  3684--3692.

\bibitem{mctr13}
M.~B. McCoy and J.~A. Tropp, ``The achievable performance of convex demixing,''
  Sep. 2013.

\bibitem{Yang2013-pf}
E.~Yang and P.~K. Ravikumar, ``Dirty statistical models,'' in \emph{Advances in
  Neural Information Processing Systems 26}, C.~J.~C. Burges, L.~Bottou,
  M.~Welling, Z.~Ghahramani, and K.~Q. Weinberger, Eds.\hskip 1em plus 0.5em
  minus 0.4em\relax Curran Associates, Inc., 2013, pp. 611--619.

\bibitem{jrsr10}
A.~Jalali, P.~Ravikumar, S.~Sanghavi, and C.~Ruan, ``{A Dirty Model for
  Multi-task Learning},'' in \emph{Advances in Neural Information Processing
  Systems}, 2010, pp. 964--972.

\bibitem{Zhang2017-rm}
Y.~Zhang and Q.~Yang, ``A survey on {Multi-Task} learning,'' 2017.

\bibitem{grti16}
S.~M. Gross and R.~Tibshirani, ``Data shared lasso: A novel tool to discover
  uplift,'' \emph{Computational Statistics \& Data Analysis}, vol. 101, pp.
  226--235, 2016.

\bibitem{asiaee2018high}
A.~Asiaee, S.~Oymak, K.~R. Coombes, and A.~Banerjee, ``High dimensional data
  enrichment: Interpretable, fast, and data-efficient,'' \emph{arXiv preprint
  arXiv:1806.04047}, 2018.

\bibitem{asiaeedata}
------, ``Data enrichment: Multi-task learning in high dimension with
  theoretical guarantees,'' in \emph{Adaptive and Multitask Learning Workshop
  at ICML}, 2019.

\bibitem{Chen2015-fj}
A.~Chen, A.~B. Owen, and M.~Shi, ``\BIBforeignlanguage{en}{Data enriched linear
  regression},'' \emph{\BIBforeignlanguage{en}{Electronic journal of
  statistics}}, vol.~9, no.~1, pp. 1078--1112, 2015.

\bibitem{domu16}
F.~Dondelinger, S.~Mukherjee, and {Alzheimer's Disease Neuroimaging
  Initiative}, ``\BIBforeignlanguage{en}{The joint lasso: high-dimensional
  regression for group structured data},''
  \emph{\BIBforeignlanguage{en}{Biostatistics}}, Sep. 2018.

\bibitem{olvi14}
E.~Ollier and V.~Viallon, ``Joint estimation of {$K$} related regression models
  with simple {$L_1$-norm} penalties,'' Nov. 2014.

\bibitem{olvi15}
------, ``Regression modeling on stratified data with the lasso,'' Aug. 2015.

\bibitem{Kakade2010-st}
S.~Kakade, O.~Shamir, K.~Sindharan, and A.~Tewari, ``Learning exponential
  families in {High-Dimensions}: Strong convexity and sparsity,'' in
  \emph{Proceedings of the Thirteenth International Conference on Artificial
  Intelligence and Statistics}, ser. Proceedings of Machine Learning Research,
  Y.~W. Teh and M.~Titterington, Eds., vol.~9.\hskip 1em plus 0.5em minus
  0.4em\relax Chia Laguna Resort, Sardinia, Italy: PMLR, 2010, pp. 381--388.

\bibitem{negahban2012restricted}
S.~Negahban and M.~J. Wainwright, ``Restricted strong convexity and weighted
  matrix completion: Optimal bounds with noise,'' \emph{Journal of Machine
  Learning Research}, vol.~13, no. May, pp. 1665--1697, 2012.

\bibitem{Plan2013-nx}
Y.~Plan and R.~Vershynin, ``Robust 1-bit compressed sensing and sparse logistic
  regression: A convex programming approach,'' \emph{IEEE transactions on
  information theory / Professional Technical Group on Information Theory},
  vol.~59, no.~1, pp. 482--494, 2013.

\bibitem{Plan2016-de}
------, ``The generalized lasso with {Non-Linear} observations,'' \emph{IEEE
  transactions on information theory / Professional Technical Group on
  Information Theory}, vol.~62, no.~3, pp. 1528--1537, Mar. 2016.

\bibitem{Yang2016-zd}
Z.~Yang, Z.~Wang, H.~Liu, Y.~Eldar, and T.~Zhang, ``Sparse nonlinear
  regression: Parameter estimation under nonconvexity,'' in \emph{Proceedings
  of The 33rd International Conference on Machine Learning}, ser. Proceedings
  of Machine Learning Research, M.~F. Balcan and K.~Q. Weinberger, Eds.,
  vol.~48.\hskip 1em plus 0.5em minus 0.4em\relax New York, New York, USA:
  PMLR, 2016, pp. 2472--2481.

\bibitem{Chen2012-fb}
J.~Chen, J.~Liu, and J.~Ye, ``\BIBforeignlanguage{en}{Learning incoherent
  sparse and {Low-Rank} patterns from multiple tasks},''
  \emph{\BIBforeignlanguage{en}{ACM transactions on knowledge discovery from
  data}}, vol.~5, no.~4, p.~22, 2012.

\bibitem{vers12}
R.~Vershynin, ``{Introduction to the non-asymptotic analysis of random
  matrices},'' in \emph{Compressed Sensing}.\hskip 1em plus 0.5em minus
  0.4em\relax Cambridge University Press, Cambridge, 2012, pp. 210--268.

\bibitem{vershynin2018high}
------, \emph{High-dimensional probability: An introduction with applications
  in data science}.\hskip 1em plus 0.5em minus 0.4em\relax Cambridge University
  Press, 2018, vol.~47.

\bibitem{banerjee14}
A.~Banerjee, S.~Chen, F.~Fazayeli, and V.~Sivakumar, ``Estimation with {Norm}
  {Regularization},'' in \emph{Advances in {Neural} {Information} {Processing}
  {Systems}}, 2014, pp. 1556--1564.

\bibitem{nrwy12}
S.~N. Negahban, P.~Ravikumar, M.~J. Wainwright, and B.~Yu, ``{A Unified
  Framework for High-Dimensional Analysis of {\$}M{\$}-Estimators with
  Decomposable Regularizers},'' \emph{Statistical Science}, vol.~27, no.~4, pp.
  538--557, 2012.

\bibitem{raskutti10}
G.~Raskutti, M.~J. Wainwright, and B.~Yu, ``Restricted eigenvalue properties
  for correlated gaussian designs,'' \emph{Journal of Machine Learning
  Research}, vol.~11, pp. 2241--2259, 2010.

\bibitem{mend15}
S.~Mendelson, ``Learning without concentration,'' \emph{Journal of the ACM},
  vol.~62, no.~3, pp. 21:1--21:25, 2015.

\bibitem{trop15}
J.~A. Tropp, ``Convex recovery of a structured signal from independent random
  linear measurements,'' in \emph{Sampling Theory, a Renaissance}.\hskip 1em
  plus 0.5em minus 0.4em\relax Springer, 2015, pp. 67--101.

\bibitem{ruzh13}
M.~Rudelson and S.~Zhou, ``{Reconstruction from anisotropic random
  measurements},'' \emph{IEEE Transactions on Information Theory}, vol.~59,
  no.~6, pp. 3434--3447, 2013.

\bibitem{boucheron13}
S.~Boucheron, G.~Lugosi, and P.~Massart, \emph{Concentration {Inequalities}:
  {A} {Nonasymptotic} {Theory} of {Independence}}.\hskip 1em plus 0.5em minus
  0.4em\relax Oxford University Press, 2013.

\bibitem{bickel2009simultaneous}
P.~J. Bickel, Y.~Ritov, A.~B. Tsybakov \emph{et~al.}, ``Simultaneous analysis
  of lasso and dantzig selector,'' \emph{The Annals of Statistics}, vol.~37,
  no.~4, pp. 1705--1732, 2009.

\bibitem{candes2007dantzig}
E.~Candes, T.~Tao \emph{et~al.}, ``The dantzig selector: Statistical estimation
  when p is much larger than n,'' \emph{The Annals of Statistics}, vol.~35,
  no.~6, pp. 2313--2351, 2007.

\bibitem{venkat12}
V.~Chandrasekaran, B.~Recht, P.~A. Parrilo, and A.~S. Willsky, ``The convex
  geometry of linear inverse problems,'' \emph{Foundations of Computational
  Mathematics}, vol.~12, no.~6, pp. 805--849, 2012.

\bibitem{chatterjee2014generalized}
S.~Chatterjee, S.~Chen, and A.~Banerjee, ``Generalized dantzig selector:
  Application to the k-support norm,'' in \emph{Advances in Neural Information
  Processing Systems}, 2014, pp. 1934--1942.

\bibitem{Vaart1996-zl}
A.~W. v.~d. Vaart and J.~A. Wellner, \emph{Weak Convergence and Empirical
  Processes: With Applications to Statistics}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, New York, NY, 1996.

\bibitem{Ledoux1991-ix}
M.~Ledoux and M.~Talagrand, \emph{Probability in Banach Spaces: Isoperimetry
  and Processes}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, Berlin,
  Heidelberg, 1991.

\bibitem{oyrs15}
S.~Oymak, B.~Recht, and M.~Soltanolkotabi, ``Sharp time--data tradeoffs for
  linear inverse problems,'' \emph{IEEE Transactions on Information Theory},
  vol.~64, no.~6, pp. 4129--4158, 2017.

\end{thebibliography}
