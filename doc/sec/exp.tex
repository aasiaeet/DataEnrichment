\section{Synthetic Experiments}
\label{sec:expds}
We considered sparsity based simulations with varying $G$ and sparsity levels. In our first set of simulations, we set $p=100$, $G=10$ and sparsity of the private parameteres to be $s=10$. We generated a dense $\bbeta_0$ with $\|\bbeta_0\|=p$ and did not impose any constraint. Iterates $\{\bbeta^{(t)}_g\}_{g=1}^G$ are obtained by projection onto the $\ell_1$ ball $\|\bbeta_g\|_1$. Nonzero entries of $\bbeta_g$ are generated with ${\cal{N}}(0,1)$ and nonzero supports are picked uniformly at random. Inspired from our theoretical step size choices, in all experiments, we used simplified learning rates of $\frac{1}{n}$ for $\bbeta_0$ and $\frac{1}{\sqrt{nn_g}}$ for $\bbeta_g$, $g\geq 1$. Observe that, cones of the individual parameters intersect with that of $\bbeta_0$ hence this setup actually violates DEIC (which requires an arbitrarily small constant fraction of groups to be non-intersecting). Our intuition is that the individual parameters are mostly incoherent with each other and the existence of a nonzero perturbation over $\bbeta_g$'s that keeps all measurements intact is unlikely. Remarkably, experimental results still show successful learning of all parameters from small amount of samples. We picked $n_g=60$ for each group. Hence, in total, we have $11p=1100$ unknowns, $200=G\times 10+100$ degrees of freedom and $G\times 60=600$ samples. In all figures, we study the normalized squared error $\frac{\|\bbeta^{(t)}_g-\bbeta_g\|_2^2}{\|\bbeta_g\|_2^2}$ and average $10$ independent realization for each curve. Figure \ref{fig syn1a} shows the estimation performance as a function of iteration number $t$. While each group might behave slightly different, we do observe that all parameters are linear converging to ground truth.% is evident from the linear slope of the $y$-axis.
	
In Figure \ref{fig syn1b}, we test the noise robustness of our algorithm. We add a ${\cal{N}}(0,1)$ noise to the $n_1=60$ measurements of the first group \emph{only}. The other groups are left untouched. While all parameters suffer nonzero estimation error, we observe that, the global parameter $\bbeta_0$ and noise-free groups $\{\bbeta_g\}_{g=2}^G$ have substantially less estimation error. This implies that noise in one group mostly affects itself rather than the global estimation. In Figure \ref{fig syn2a}, we increased the sample size to $n_g=150$ per group. We observe that, in comparison to Figure \ref{fig syn1a}, rate of convergence receives a boost from the additional samples as predicted by our theory.
	
	
Finally, Figure \ref{fig syn2b} considers a very high-dimensional problem where $p=1000$, $G=100$, individual parameters are $10$ sparse, $\bbeta_0$ is $100$ sparse and $n_g=150$. The total degrees of freedom is $1100$, number of unknowns are $101000$ and total number of datapoints are $150\times 100=15000$. While individual parameters have substantial variation in terms of convergence rate, at the end of $1000$ iteration, all parameters have relative reconstruction error below $10^{-6}$.
%	\begin{figure}[t!]
%		\begin{subfigure}[b]{0.5\textwidth}
%			\includegraphics[width=\textwidth]{img/betag_converge_G10_p100.eps}
%			
%			\caption{}\label{fig syn1a}
%		\end{subfigure} ~
%		\begin{subfigure}[b]{0.5\textwidth}
%			\includegraphics[width=\textwidth]{img/betag_converge_noise_G10_p100.eps}
%			
%			\caption{}\label{fig syn1b}
%		\end{subfigure}
%		\caption{a) Noiseless fast convergence. b) Noise on the first group does not impact other groups as much.}
%		\label{fig syn1}
%	\end{figure}
%	
%	\begin{figure}[t!]
%		\begin{subfigure}[b]{0.5\textwidth}
%			\includegraphics[width=\textwidth]{img/betag_converge_G10_p100_fast.eps}
%			
%			\caption{} \label{fig syn2a}
%		\end{subfigure} ~
%		\begin{subfigure}[b]{0.5\textwidth}
%			\includegraphics[width=\textwidth]{img/betag_converge_G100_p1000_shrink}
%			
%			\caption{}\label{fig syn2b}
%		\end{subfigure}
%		\caption{a) Increasing sample size improves rate of convergence. b) Our algorithm convergences fast even with a large number of groups $G=100$.}
%		\label{fig syn2}
%	\end{figure}

\vspace{5mm}

		\begin{figure}[t!]
			\begin{subfigure}[b]{0.23\textwidth}
				\includegraphics[width=\textwidth]{./img/betag_converge_G10_p100.pdf}
				
				\caption{}\label{fig syn1a}
			\end{subfigure} ~
			\begin{subfigure}[b]{0.23\textwidth}
				\includegraphics[width=\textwidth]{./img/betag_converge_noise_G10_p100.pdf}
				
				\caption{}\label{fig syn1b}
			\end{subfigure}
%			\label{fig syn1}
			\begin{subfigure}[b]{0.23\textwidth}
				\includegraphics[width=\textwidth]{./img/betag_converge_G10_p100_fast.pdf}
				\caption{} \label{fig syn2a}
			\end{subfigure} ~
			\begin{subfigure}[b]{0.23\textwidth}
				\includegraphics[width=\textwidth]{./img/betag_converge_G100_p1000_shrink.pdf}
				\caption{}\label{fig syn2b}
			\end{subfigure}
			\caption{a) Noiseless fast convergence. b) Noise on the first group does not impact other groups as much. c) Increasing sample size improves rate of convergence. d) Our algorithm convergences fast even with a large number of groups $G=100$.}
			\label{fig syn2}
		\end{figure}

%%%%%%%%%%%%% Amir's exp commented for now
%In this section we supplement our theoretical results with a simple synthetic experiment. 
%We focus on the case of two groups, i.e., $G = 2$. 
%The dimension $p = 1000$ and the structure is sparsity induced by $l_1$-norm. 
%The parameters $\bbeta _0^*$, $\bbeta _1^*$, and $\bbeta _2^*$ are 20, 10, and 5-sparse respectively. 
%The sparsity pattern is as follows:$\bbeta _0^* = (\underbrace{1, \dots, 1}_{1-20}, 0, \dots)$,$\bbeta _1^* = (\dots, 0, \underbrace{2, \dots, 2}_{51-60}, 0, \dots)$, and $\bbeta _2^* = (\dots, 0, \underbrace{-2, \dots, -2}_{96-100}, 0, \dots)$. 
%
%For the distribution of input and noise we have $\x_{gi} \sim N(0, \sigma_x^2 \I)$ and $\omega_{gi} \sim N(0, \sigma_w^2)$ with $\sigma_x^2 = .3$ and  $\sigma_w^2 = .1$.
%We use the SPGD method (Algorithm \ref{alg2}) to solve the optimization problem \eqref{eq:compact}. 
%The projection to the $l_1$ ball can be efficiently performed by the method proposed in \cite{dssc08}. 
%
%While changing $n$ in the experiments, we keep the ratio $\frac{n_1}{n_2} = \frac{2}{3}$ fixed. 
%Figures \ref{fig:individual1} and \ref{fig:individual2} show the per-group error for different sample size which follows $1/\sqrt{n_g}$ decay.
%Finally, Figure \ref{fig:common} shows the decay of the error as sample size increases for the common component recovery and the error for summation of the form \eqref{eq:errorsum}.
%As expected errors decay as $1/\sqrt{n}$.
%
%\begin{figure}
%	\centering
%	\subcaptionbox{
%		Individual parameter one.
%		\label{fig:individual1}
%	}{\includegraphics[width=0.3 \textwidth]{./img/synthbeta1}}~
%	\subcaptionbox{
%		Individual parameter two.
%		\label{fig:individual2}
%	}{\includegraphics[width=0.3 \textwidth,]{./img/synthbeta2}}
%	\subcaptionbox{
%		Common parameter.
%	\label{fig:common}
%	}{\includegraphics[width=0.3 \textwidth,]{./img/synthbeta0}}
%	\caption{Estimation error with different sample size. Each point on the diagram is an average over 30 experiments.} %\ref{fig:common} compares the error with the LHS of \eqref{eq:errorsum}
%	\label{fig:gasprices}
%\end{figure}

\vspace{-10mm}
\section{Drug Sensitivity Analysis for Cancer Cell Lines}
\label{realexp}
In this section we investigate the application of DS in analyzing the response of patients with cancer to different doses of various drugs. 
Each cancer type (lung, blood, etc.) is a group $g$ in our DS model and the respond of patient $i$ with cancer $g$ to the drug is our output $y_{gi}$. 
The set of features for each patient $\x_{gi}$ consists of gene expressions, copy number variation, and mutations and $y_{gi}$ is the ``activity area'' above the dose-response curve, Figure \ref{fig:dr}.
Given $\x_{gi}$ and a drug, we have two goals: accurately predict a patient's response to the drug and identifying genetic predictors of drug sensitivity. 
%In other words, we are interested in both predicting of $y_{gi}$ and identifying feature $j$ with highest absolute $\bbeta_{g}(j)$ value for each cancer $g$.
%We hope that the data enriched model captures the important shared feature between cancers, as long as the cancer-specific genetic factors that determine the response to therapy.
%In the following, we first explain the dataset and then compare the prediction performance of DS with elastic net on 24 drugs. 
%Then, we perform a more detailed study of the features selected by DS to validate if DS extracts meaningful feature for each cancer. 
%\subsection{Data Set}
%A dose-response dataset of an anticancer drug $d$ and a cell line $gi$ (sampled from a patient $i$ with cancer $g$) consists of pairs ${(c, p)}$ which means proportion $p$ of cells of the cell line $gi$ are alive (relative viability) when treated with drug $d$ with concentration $c$. 
%%There are many methods that fit different function to dose-response data, 
%When a drug \emph{works}, after fitting a function to raw data, we will have a reverse sigmoid-like graph, e.g., Figure \ref{fig:dr}. 
%An effective drug kills more cells with lower concentrations therefore its  ``activity area'' \cite{zoha05}, i.e., area above the curve in Figure \ref{fig:dr}, is larger.
%%There are many ways to measure the drug efficacy from the dose-response curve. 
%We use the activity area of the drug for cell line $gi$ as our output of interest $y_{gi}$. 
%
%\begin{figure}
%	\centering
%	\includegraphics[width=0.3 \textwidth,]{./img/dr4}
%	\caption{}
%	\label{fig:dr}
%\end{figure}

%In this experiment 
We use Cancer Cell Line Encyclopedia (CCLE) \cite{barretina2012cancer} which is a compilation $\sim$500 human cancer cell lines where responses of them to 24 anticancer drugs have been measured.%\footnote{Here the technical meaning of response $y_{gi}$ is the ``activity area'' above the dose-response curve\cite{barretina2012cancer}.}.
%The goal of the study was to identify genetic predictor of drug sensitivity for different cancers. 
%The method used for analysis of CCLE in the original paper was Elastic Net \cite{zoha05} which we use as the comparison baseline. 
%\footnote{Cancer type is the first tissue where the cancer was observed.} 
From the 36 cancer type available in CCLE, we focus on lung and blood.%\footnote{By blood cancer, we mean any cancer originate from haematopoietic and lymphoid tissues.} 
Not all of the ~500 lines have been treated with all of the drugs. 
Therefore we end up with a different number of samples $n$ for each drug where the range is $n \in [150, 200]$.
Also, we perform a standard preprocessing \cite{barretina2012cancer} where we remove features with less than $.1$ absolute correlation with the response of interest which reduce the dimension to $p \in [1500, 5000]$ range. 
%Since the features that get removed are different for each drug, the dimension in the 24 problems is also variable, where the range is $p \in [1500, 5000]$. 

%\subsection{Results}
{\bf Prediction:} Here we run, 24 different experiments, each for one drug.  %= \{f_g(\bbeta) < b_g\}$
Since the values of $d_g$  in constraint sets $\Omega_{f_g}(d_g)$  are unknown, we tune them by 10-fold cross-validation and report the mean squared error (MSE) of the Elastic Net \cite{zoha05} (method used in the original CCLE paper\cite{barretina2012cancer}) and the data enrichment in Figure \ref{fig:prediction}. Both methods have very close prediction performance.
	\begin{figure}[t]
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth]{./img/dr4.pdf}
		\caption{}\label{fig:dr}
	\end{subfigure} ~
	\begin{subfigure}[b]{0.30\textwidth}
		\includegraphics[width=\textwidth]{./img/MSEComparison.pdf}
		\caption{}\label{fig:prediction}
	\end{subfigure}
	\begin{subfigure}[b]{0.29\textwidth}
	\includegraphics[width=\textwidth,]{./img/Saracatinib.pdf}
	\caption{}\label{fig:Saracatinib}
	\end{subfigure}
	\caption{a) A sample fitted dose-response curve where Activity Area $y_{gi}$ is shaded. b) Comparison of Mean Square Error of elastic net and data enrichment in predicting the response to 24 drugs for lung and blood cancer cell lines. Each dot represents an experiment for a drug. Prediction accuracy of both algorithms are very close. c) Distribution of responses to Saracatinib.}
	\label{fig syn2}
	\end{figure}

{\bf Interpretation}
%Next we focus on the interpretation of the selected genes. 
We select Saracatinib, a drug that works on both lung and blood cancers, Figure \ref{fig:Saracatinib}. 
Fixing the $d_g$ parameters, we select the genes which have non-zero coefficient 40 times across 50 runs of PBGD on bootstrapped samples.
Now, we have three lists of genes based on the supports of shared , lung , and blood parameters. 
We perform gene enrichment analysis using ToppGene \cite{chen09toppgene} to see where in functional/disease/drug databases these genes have been observed together with statistical significance. 
Table \ref{table:1} summarizes a highlight of our findings which shows lung and blood parameters are correctly capturing a meaningful set of genes.% while the shared parameter has mixed set of genes. 
%{\small 
%\begin{tabular}{ |c|c|c|c|c|c|  }
%	\hline 	
%	\multicolumn{2}{|c|}{(Blood, 512)} & \multicolumn{2}{c|}{(Lung, 500)} & \multicolumn{2}{c|}{(Shared, 525)}\\
%	\hline
%	Highlights &    p-Val  & Highlights &    p-Val &  Highlights &    p-Val \\
%	\hline
%	regulation of immune response	 & 2.1E-8  & Secondary malignant neoplasm of lung & 8.9E-6  &  Acute	Myeloid Leukemia & 5.0E-7 \\
%	T cell activation	 & 5.0E-8   & Lung Cancer  & 2.9E-5  &  Chronic Myeloid Leukemia & 3.0E-5 \\
%	leukocyte activation & 1.0E-6  & Adenosquamous cell lung cancer	 & 3.9E-5  &  Adenocarcinoma of lung & 4.8E-5 \\
%	\hline
%\end{tabular}
%}
\begin{table}
	\centering
	\begin{tabular}{ |c|c|c|c|  }
		\hline 	
		\multicolumn{2}{|c|}{(Blood, 512)} & \multicolumn{2}{c|}{(Lung, 500)}\\
		\hline
		Highlights &    p-Val  & Highlights &    p-Val   \\
		\hline
		Regulation of immune response	 & 2.1E-8  & Secondary malignant neoplasm of Lung & 8.9E-6  \\
		T cell activation	 & 5.0E-8   & Lung cancer  & 2.9E-5   \\
		Leukocyte activation & 1.0E-6  & Adenosquamous cell lung cancer	 & 3.9E-5 \\
		\hline
	\end{tabular}
\caption{Each column is (Cancer Type, Number of significant genes) and highlights show where the set of genes have been observed together. p-Values are computed by  Fisher's exact test \cite{chen09toppgene}.}
\label{table:1}
\end{table}

%{ (Type, $|\text{supp}(\cdot)|$)} 	&   , ; 2.907E-5  & asd;flkajsdf ;as  &  asdf'alsdkfad' \\
%&   , stage IV;   & asd;flkajsdf ;as  &  asdf'alsdkfad' \\