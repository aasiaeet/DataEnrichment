
\section{Estimation Algorithm}
\label{sec:opt}
We propose the following Projected Block Gradient Descent algorithm (PBGD), Algorithm \ref{alg2}, where $\Pi_{\Omega_{f_g}}$ is the Euclidean projection onto the set $\Omega_{f_g}(d_g) = \{f_g(\bbeta) \leq d_g\}$ where $d_g = f_g(\bbeta_g^*)$ and is dropped to avoid cluttering. In practice, $d_g$ can be determined by cross-validation.

\begin{algorithm}[t]
	\caption{  PBGD: {\sc Projected Block Gradient Descent}}
	\label{alg2}
	\begin{algorithmic}[1]
		\STATE {\bfseries input:} $\X, \y$, learning rates $(\mu_0, \dots, \mu_G)$, initialization $\bbeta ^{(1)} = \0$
		\STATE {\bfseries output:} $\hbbe$
		\FOR{t = 1 \TO T}
		\FOR{g=1 \TO G}
		\STATE {\tiny $\bbeta _g^{(t+1)} = \Pi_{\Omega_{f_g}} \left(\bbeta _g^{(t)} + \mu_g \X_g^T \left(\y_g - \X_g \left(\bbeta _0^{(t)} + \bbeta _g^{(t)}\right) \right) \right)$}
		\ENDFOR
		\STATE {\tiny $\bbeta _0^{(t+1)} = \Pi_{\Omega_{f_0}} \left(\bbeta _0^{(t)} + \mu_0 \X_0^T \left(\y - \X_0 \bbeta _0^{(t)} -
		\begin{pmatrix}
		\X_1 \bbeta _1^{(t)}      \\
		\vdots 	 \\
		\X_G  \bbeta _G^{(t)}
		\end{pmatrix}\right)\right)$}
		\ENDFOR
	\end{algorithmic}
\end{algorithm}


%In remark ?, we discuss the deficiency of the method presented in \cite{oyrs15} applied to data enrichment problem.
%We suggest $\mu_g = \frac{1}{n_g}$ as the per group learning rate and $\mu_0 = \frac{1}{n}$ for the step-size of the common parameter update.
%In the following section, we show with these learning rates, Algorithm \ref{alg2} converges exponentially fast.

%{\color{red} TODO: Intuitively explain why this choice makes sense.}
\subsection{Convergence Rate Analysis}
\ab{Unless we need a second subsection, drop this subsection heading -- its unusual to have one subsection in a section.}

Here, we want to upper bound the error of each iteration of the PBGD algorithm.
Let's $\ddelta^{(t)} = \bbeta^{(t)} - \bbeta^*$ be the error of  iteration $t$ of PBGD, i.e., the distance from the true parameter (not the optimization minimum, $\hbbe$).
%The goal of this chapter is to show that the $\sum_{g = 1}^{G} \sqrt{\frac{n_g}{n}} \norm{\ddelta_g^t}{2}$ decreases exponentially fast it $t$, and converges to the upper bound of the statistical error $\sum_{g = 1}^{G} \sqrt{\frac{n_g}{n}} \norm{\ddelta_g}{2}$ characterized in \eqref{eq:general}.
We show that $\norm{\ddelta^{(t)}}{2}$ decreases exponentially fast in $t$ to the statistical error $\norm{\ddelta}{2} = \norm{\hbbe - \bbeta^*}{2}$.
%In other word, we show that the optimization error $\norm{\hbbe - \bbeta^(t)}{2}$ linearly converges to zero.
We first start with the required definitions for our analysis.

\begin{definition}
	\label{def:only}
	We define the following positive constants as functions of step sizes $\mu_g > 0$: %, where for simplification we assume $\X_0 = \oomega$ and $\oomega_0 = \oomega$:
%	\be
%	\nr
%	\rho_g(\mu_g) &=& \sup_{\u, \v \in \cB_g} \v^T \big(\I_g - \mu_g \X_g^T \X_g\big) \u, \quad g \in [G] \\ \nr
%	\eta_g(\mu_g) &=& \mu_g \sup_{\v \in \cB_g} \v^T \X_g^T \frac{\oomega_g}{\norm{\oomega_g}{2}}, \quad g \in [G] \\ \nr
%	\phi_g(\mu_g) &=& \mu_g \sup_{\v \in \cB_g, \u \in \cB_0} -\v^T \X_g^T \X_g \u, \quad g \in [G]_\setminus
%	\ee
	\be
	\nr
	\forall g \in [G]&:& \rho_g(\mu_g) = \sup_{\u, \v \in \cB_g} \v^T \big(\I_g - \mu_g \X_g^T \X_g\big) \u,
	\\ \nr
	&&\eta_g(\mu_g) = \mu_g \sup_{\v \in \cB_g} \v^T \X_g^T \frac{\oomega_g}{\norm{\oomega_g}{2}},
	\\ \nr
	\forall g \in [G]_\setminus&:& \phi_g(\mu_g) = \mu_g \sup_{\v \in \cB_g, \u \in \cB_0} -\v^T \X_g^T \X_g \u,
	\ee
	where $\cB_g =  \cC_g \cap \ball$ is the intersection of the error cone and the unit ball.% and $\oomega_0 := \oomega$.
\end{definition}
In the following theorem, we establish a deterministic bound on iteration errors  $\norm{\ddelta_g^{(t)}}{2}$ which depends on constants defined in Definition \ref{def:only}.
\begin{theorem}
	\label{theo:iter}
	For Algorithm \ref{alg2} initialized by $\bbeta ^{(1)} = \0$, we have the following deterministic bound for the error at iteration $t + 1$:
	{\small\be
	\label{eq:singleiter}
	&&\sum_{g=0}^{G} \sqrt{\frac{n_g}{n}} \norm{\ddelta_g^{(t+1)}}{2}
	\\ \nr
	&&\leq \rho^t \sum_{g=0}^{G}\sqrt{\frac{n_g}{n}}\norm{\bbeta ^*_g}{2}   + \frac{1 - \rho^t}{1 -  \rho}   \sum_{g=0}^{G} \sqrt{\frac{n_g}{n}} \eta_g \norm{\oomega_g}{2},
	\ee}
	where {\small$\rho \triangleq \max\left(\rho_0 + \sum_{g=1}^{G} \sqrt{\frac{n_g}{n}} \phi_g, \max_{g \in [G]} \left[\rho_g + \sqrt{\frac{n}{n_g}}  \frac{\mu_0}{\mu_g} \phi_g \right]  \right)$}.
%	\be
%	\label{eq:rhos}
%	\rho = \max\left(\rho_0 + \sum_{g=1}^{G} \sqrt{\frac{n_g}{n}} \phi_g, \max_{g \in [G]} \left[\rho_g + \sqrt{\frac{n}{n_g}}  \frac{\mu_0}{\mu_g} \phi_g \right]  \right)
%	\ee
\end{theorem}


The RHS of \eqref{eq:singleiter} consists of two terms.
If we keep $\rho < 1$, the first term approaches zero exponentially fast, i.e., with linear rate, and the second term determines the bound. %which depends on the sub-Gaussian noise $\oomega$, and therefore for noiseless case of $\y = \X \bbeta$ it becomes zero.
In the following, we show that for specific choices of step sizes $\mu_g$s, the second term can be upper bounded using the analysis of Section \ref{sec:error}.
More specifically, the first term corresponds to the optimization error which shrinks in every iteration while the second term is constant times the upper bound of the statistical error characterized in Corollary \ref{corr:calcub}.
Therefore, if we keep $\rho$ below one, the estimation error of PBGD algorithm linearly converges to the approximate statistical error bound. %and we get to $\hbbe$ exponentially fast.

%i.e., $\norm{\hbbe - \bbeta^*}{2} = \norm{\ddelta}{2} =  \sqrt{\sum_{g=1}^{G} \norm{\ddelta_g}{2}^2} \leq \sum_{g=1}^{G} \norm{\ddelta_g}{2}$ where we characterized a scaled version of it as $\sum_{g=1}^{G} \sqrt{\frac{n_g}{n}}\norm{\ddelta_g}{2}$ in \eqref{eq:general}.


One way for having $\rho < 1$ is to keep all arguments of $\max(\cdots)$  defining $\rho$ strictly below $1$. %, i.e., $\rho_g < 1/\theta_f $.
To this end, we first establish high probability upper bound for $\rho_g$, $\eta_g$, and $\phi_g$ (in the Appendix) and then show that with enough number of samples and proper step sizes $\mu_g$, $\rho$ can be kept strictly below one with high probability. In Section~\ref{sec:expds}, we empirically illustrate such geometric convergence.
The high probability bounds for constants in Definition \ref{def:only} and the deterministic bound of Theorem \ref{theo:iter} leads to the following theorem which shows for enough number of samples, of the same order as the statistical sample complexity, we can keep $\rho$ below one and have geometric convergence.





%The following theorems uses Lemmas \ref{lemm:hpub} and \ref{lemm:mainlem} and shows for enough number of samples we can keep $\theta_f \rho$ below one.
%Bounds on $\rho_g\left(\frac{1}{n_g}\right)$ and $\eta_g\left(\frac{1}{n_g}\right)$ suggest that with enough number of samples, learning rate of $\mu_g = \frac{1}{n_g}$ leads to a linear rate of convergence to a constant times the statistical error bound in \eqref{eq:singleiter}.
%The following theorem elaborates the result.
%Using Lemma \ref{lemm:hpub} and \ref{lemm:mainlem} bounds
%the following theorem shows that for a specific choice of step-sizes as $\mu_g = \frac{1}{n_g}$, $\rho_{\max} < 1$ with high probability and error of each iteration $\norm{\ddelta^{(t+1)}}{2}$ reaches to a scaled upper bound of statistical error $\norm{\ddelta}{2}$ exponentially fast.
\begin{theorem}
	\label{theo:step}		
	%Remember that $\theta_f = 1$ for the convex and  $\theta_f = 2$ for the non-convex case.
	Let $\tau = C\sqrt{\log(G+1)} + b$ for $b > 0$ and $\omega_{0g} = \omega(\cA_0) + \omega(\cA_g)$. For the per-group step sizes of:
	\be
	\nr
%	&\theta_f = 1:&
	&&\mu_0 = \frac{1}{4	n} \times \min_{g \in [G]_\setminus} \left(1 + c_{0g} \frac{\omega_{0g}+ \tau}{\sqrt{n_g}}\right)^{-2},
	\\ \nr
	&&\mu_g =  \frac{1}{2\sqrt{n n_g}} \left(1 + c_{0g}\frac{\omega_{0g}+ \tau}{\sqrt{n_g}} \right)^{-1}
%	\\ \nr
%	&\theta_f = 2:& \mu_0 = \frac{1}{4	n} \times \frac{1}{\max_{g \in [G]_\setminus} \left(1 + c_{0g} \frac{\omega_{0g}+\tau}{\sqrt{n_g}}\right)^2} , \mu_g = \frac{l\left(1 + c_{0g}\frac{\omega_{0g}+ \tau}{\sqrt{n_g}} \right)^{-1}}{8\sqrt{nn_g}\max_{g \in [G]_\setminus} \left(1 + c_{0g} \frac{\omega_{0g}+\tau}{\sqrt{n_g}}\right)^{2}}
	\ee
	and sample complexities of $\forall g \in [G]: n_g \geq 2c_g^2 (2 \omega(\cA_g) + \tau)^2$,
%	\be
%	\nr
%	%&\theta_f = 1:&
%%	n > 2 c_0^2 \left(2 \omega(\cA_0) + \tau\right)^2, \forall g \in [G]_\setminus: n_g \geq 2c_g^2 (2 \omega(\cA_g) + \tau)^2
%n > 2 c_0^2 \left(2 \omega(\cA_0) + \tau\right)^2, \quad \text{and} \quad \forall g \in [G]_\setminus: n_g \geq 2c_g^2 (2 \omega(\cA_g) + \tau)^2
%%	\\ \nr
%%	&\theta_f = 2:& n > 2 (1 - l)^{-2} c_0^2 \left(2 \omega(\cA_0) + \tau\right)^2, \forall g \in [G]_\setminus: n_g \geq 2c_g^2 (2 \omega(\cA_g) + \tau)^2
%	\ee
%	where $0 < l < 1$ is arbitrary,
	updates of the Algorithm \ref{alg2} obey the following with high probability: %probability at least $ 1 - \upsilon \exp\left[\min_{g \in [G]}\left(-\min\left[\nu_g n_g - \log G, \gamma (\omega(\cA_g) + b)^2 , \frac{b^2}{\eta_g^2 k^2}\right]\right)\right] $:
	%where $\upsilon = \max(28, \sigma)$ and $\gamma = \min_{g \in [G] } \gamma_g$. 	
	{\small\be
	\nr
	\sum_{g=0}^{G} \sqrt{\frac{n_g}{n}} \norm{\ddelta_g^{(t+1)}}{2}
	&\leq& r(\tau)^t \sum_{g=0}^{G} \sqrt{\frac{n_g}{n}} \norm{\bbeta^*_g}{2}   \\ \nr
	&+& \frac{(G+1) \sqrt{(2K^2 + 1)}}{\sqrt{n} (1 - r(\tau))}  \left(\zeta k \max_{g \in [G]} \omega(\cA_g) + \tau \right),
	\ee}
	where $r(\tau) < 1$.% and $\epsilon = \max_{g \in [G]} \epsilon_g$, $\zeta = \max_{g \in [G]} \zeta_g$, $\gamma = \min_{g \in [G]} \gamma_g$.
	
\end{theorem}

\begin{corollary}
	\label{corr:show}
	When $t \rightarrow \infty$ we have the following with high probability:
	\be
	\label{eq:scaled}
	&&\sum_{g=0}^{G} \sqrt{\frac{n_g}{n}} \norm{\ddelta_g^{\infty}}{2}
	\leq \frac{(G+1) \sqrt{(2K^2 + 1)}}{\sqrt{n} (1 - r(\tau))}
	\\  \nr
	&\times& \left(\zeta k \max_{g \in [G]} \omega(\cA_g) + C \sqrt{\log (G+1)} +  b \right),
	\ee
\end{corollary}
	It is instructive to compare RHS of \eqref{eq:scaled} with that of \eqref{eq:general}: $\kappa_{\min}$ defined in Theorem \ref{theo:re} corresponds to $(1 - r(\tau))$ % defined in Theorem \ref{theo:step}
	and the extra $G+1$ factor corresponds to the sample condition number $\gamma = \max_{g \in [G] } \frac{n}{n_g}$.
	Therefore, Corollary \ref{corr:show} shows that PBGD converges to a scaled variant of statical error bound determined in Corollary \ref{corr:calcub}.
%	The extra factor of $G+1$ can be dismissed if we have more samples and also take a more conservative step size of $\mu_g = \frac{1}{(G+1) n_g}$.
%	Following proposition states this result.
%
%\begin{prop}
%	\label{prop:2}
%	For per group step size of $\mu_g = \frac{1}{(G+1) n_g}$ and common parameter step size of $\mu_0 = \frac{1}{(G+1) n}$ when per group and total number of samples are large enough, i.e., $n_g \geq 4 \theta_f^2 c_g^2(\omega(\cA_g) + \tau)^2$ we have the following for PBGD with probability at least $ 1 - \upsilon \exp\left\{-\min_{g \in [G]} \left(\min\left[\nu_g n_g - \log G, -\gamma_g (\omega(\cA_g) + t)^2, \frac{t^2}{\eta_g^2 k^2}\right]\right) \right\}$:
%	\be
%	\nr
%	\sum_{g=0}^{G} \sqrt{\frac{n_g}{n}} \norm{\ddelta_g^{\infty}}{2}
%	\leq C \theta_f \frac{\zeta k \max_{g \in [G]} \omega(\cA_g) + \epsilon \sqrt{\log G} +  \tau}{\sqrt{n} (1 - r)}  ,
%	\ee
%	where $C = \sqrt{(2K^2 + 1)}$.
%\end{prop}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Samet Part Begins %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Convergence with minimal samples using mild learning rates}
%
%\begin{lemma} \label{spectral lem}Suppose $n_g\geq CK^4\omega^2(\cA_g)$ for all $g$. For all $\X_i\in\R^{n_i\times p_i}$ (after scaling by $1/\sqrt{n}$) and $\ddelta\in \cC_i$, with probability $1-\sum_{g=0}^G\exp(-cn_i)$, we have that
%	\[
%	\tn{\X_i\ddelta_i}\leq 2\sqrt{n_i/n}\tn{\ddelta_i},~\|\X_i\|\leq (\sqrt{n_i/n}+C'K^2\sqrt{p_i/n}),~\tn{\X_i^T\X_i\ddelta_i}^2\leq \Lambda_i.
%	\]
%	where $\Lambda_i=4(n_i/n)(\sqrt{n_i/n}+C'K^2\sqrt{p_i/n})^2$.
%\end{lemma}
%\begin{proof} These are fairly standard results. For instance, the reader is referred to Theorem $1.1$ of Liaw et al. \cite{liaw2017simple} or \cite{oymak2015isometric}.
%\end{proof}
%The next theorem provides our main result on convergence with near optimal sample complexity where $n_g\geq C\omega^2(\cA_g)$ per group. We assumed a noiseless model to have a cleaner presentation which keeps the result more insightful.
%\begin{theorem} Consider the modified PBGD algorithm where line 7 uses $\beta_g^{(t)}$ instead of $\beta_g^{(t+1)}$. Assume sample sizes $n_i$ are set same as in Theorem \ref{theo:re}. Set $\kappa=\kappa_{\min}^2/4$. Let $r_g=G n_g/n$ and $r_0=1$. Set learning rates to be $\mu_g=\mu$ for $g\geq 1$ and $\mu_0=G^{-2}\mu$ where
%	\begin{align}
%	\mu=c{\kappa}\min_{1\leq g\leq G}\{\frac{1}{G\max\{1,K^4p_g/n_g\}},\frac{1}{\max\{1,K^4p_0/n\}} \}\label{learning rate rule}
%	\end{align}
%	Define distance metric as
%	\[
%	{\bf{d}}(\ddelta)=G^2\tn{\ddelta_0}^2+\sum_{g=1}^G \tn{\ddelta_g}^2.
%	\]
%	With probability $1-\sum_{g=0}^G\exp(-cn_i)-\exp(-\kappa n/2)$, for all $t\geq 0$, we have that
%	\[
%	{\bf{d}}(\ddelta^{(t+1)})\leq (1-\kappa\mu\min_{1\leq g\leq G}\frac{n_g^2}{n^2}){\bf{d}}(\ddelta^{(t)}).
%	%G^3\tn{\ddelta_0}^2+\sum_{g=1}^G  \tn{\ddelta_g^{(t+1)}}^2\leq (1-\rho) (G^3\tn{\ddelta_0}^2+\sum_{g=1}^G \tn{\ddelta_g^{(t)}}^2).
%	\]
%\end{theorem}
%Assuming groups are of the same size and $\alpha=p_g/n_g$ is a constant, rate of convergence $\rho$ simplifies to
%\[
%\rho\sim1-\frac{1}{G^3\max\{1,\alpha\}}
%\]
%This means that, compared to traditional least squares regression, we are losing a factor of $G^3$ in terms of convergence rate. Additionally, we converge not directly in $\ell_2$ distance but rather scaled $\ell_2$ distance where contribution of $\ddelta_0$ is heavier than the rest.
%\begin{proof}
%	PBGD iterations imply that
%	\[
%	\ddelta_g^{t+1}=\Pi_g(\ddelta_g-\mu \X_g^T\X_g(\ddelta_0^t+\ddelta_g^t))
%	\]
%	\[
%	\ddelta_0^{t+1}=\Pi_0(\ddelta_0-\mu_0 \sum_{g=1}^G\X_g^T\X_g(\ddelta_0^t+\ddelta_g^t))
%	\]
%	which implies $\tn{\ddelta_g^{t+1}}\leq \tn{\ddelta_g-\mu \X_g^T\X_g(\ddelta_0^t+\ddelta_g^t)}$. On the other hand, setting $\mu_0=C^{-1}G\mu$
%	\[
%	\tn{\ddelta_g-\mu \X_g^T\X_g(\ddelta_0^t+\ddelta_g^t)}^2=\tn{\ddelta_g}^2-2\mu \ddelta_g^T\X_g^T\X_g(\ddelta_0+\ddelta_t)+\mu^2\tn{\X_g^T\X_g(\ddelta_0^t+\ddelta_g^t)}^2
%	\]
%	\[
%	 C\tn{\ddelta_0^{t+1}}^2=C\tn{\ddelta_0}^2-2G\mu\sum_g\ddelta_0^T\X_g^T\X_g(\ddelta_0+\ddelta_t)+C^{-1}G^2\mu^2\tn{\sum_{g=1}^G\X_g^T\X_g(\ddelta_0+\ddelta_t)}^2.
%	\]
%	We will study the linear combination $C\tn{\ddelta_0}^2+\sum_{g=1}^G \tn{\ddelta_g^{t+1}}^2$. where the $\ddelta_0$ component is scaled by $C$. Denote terms without $\mu$ multiplier by $S_0$, with $\mu$ multiplier by $S_1$ and with $\mu^2$ multiplier by $S_2$.
%	%Also pick $C_g=n_g^2$.
%	Scaling $\ddelta_0$ component by $C$ and summing over all, we find
%	\begin{align}
%	S_0+S_1&=C\tn{\ddelta_0}^2+\sum_{g=1}^G \tn{\ddelta_g}^2-2\sum_{g=1}^G \mu (\ddelta_0+\ddelta_t)^T\X_g^T\X_g(\ddelta_0+\ddelta_t)\\
%	&=C\tn{\ddelta_0}^2+\sum_{g=1}^G \tn{\ddelta_g}^2-\sum_{g=0}^G \mu (\ddelta_0+\ddelta_t)^T\X_g^T\X_g(\ddelta_0+\ddelta_t)\\
%	&\geq C\tn{\ddelta_0}^2+\sum_{g=1}^G \tn{\ddelta_g}^2-\mu \kappa(\sum_{g=0}^G \frac{n_g}{n}\tn{\ddelta_g})^2
%	\end{align}
%	where we applied Theorem \ref{theo:re}. Next, observe that
%	\begin{align}
%	&\tn{\X_g^T\X_g(\ddelta_0^t+\ddelta_g^t)}^2\leq 2(\tn{\X_g^T\X_g\ddelta_0^t}^2+\tn{\X_g^T\X_g\ddelta_g^t}^2).\\
%	&\tn{\sum_{g=1}^G\X_g^T\X_g(\ddelta_0+\ddelta_t)}^2\leq 2(\tn{\X_0^T\X_0\ddelta_0}^2+\tn{\sum_{g=1}^G\X_g^T\X_g\ddelta_t}^2)\leq 2(\tn{\X_0^T\X_0\ddelta_0}^2+G\sum_{g=1}^G\tn{\X_g^T\X_g\ddelta_t}^2)
%	\end{align}
%	Hence, summing over the $\mu^2$ terms and using $\mu_0=C^{-1}G\mu$, for $g\geq 1$, and assuming $C\leq G^2$, we have
%	\begin{align}
%	S_2&\leq 2\mu^2((1+G^3C^{-1})\sum_{g=1}^G\tn{\X_g^T\X_g\ddelta_t}^2+(1+G^2C^{-1})\tn{\X_0^T\X_0\ddelta_0}^2)\\
%	&\leq4\mu^2G^2C^{-1}(G\sum_{g=1}^G\tn{\X_g^T\X_g\ddelta_t}^2+\tn{\X_0^T\X_0\ddelta_0}^2)
%	\end{align}
%	Applying Lemma \ref{spectral lem}, coefficient of $\tn{\delta_g^{(t+1)}}^2$ and $\tn{\delta_0^{(t+1)}}^2$, within the sum $S_0+S_1+S_2$, are given by% and setting $\gamma_1=\sum_{g=1}^G\tn{\X_g^T\X_g\ddelta_t}^2
%	\begin{align}
%	&\tn{\delta_g^{(t+1)}}^2\leq \tn{\delta_g^t}^2(1-\mu \kappa \frac{n_g^2}{n^2}+4\mu^2G^3C^{-1}\Lambda_g).\\
%	&C\tn{\delta_0^{(t+1)}}^2\leq C\tn{\delta_0^t}^2(1-C^{-1}\mu \kappa +4\mu^2G^2C^{-2}\Lambda_0).
%	\end{align}
%	This implies picking the step size
%	\[
%	\mu=\frac{\kappa}{8} \min\{\frac{Cn_g^2}{n^2G^3\Lambda_g},\frac{C}{G^2\Lambda_0}\}).
%	\]
%	Observing $\Lambda_g=c'n^{-2}n_g^2\max\{1,K^4p_g/n_g\}$, this yields% and setting $C=G^2$
%	\[
%	\mu=cC{\kappa}\min_{1\leq g\leq G}\{\frac{1}{G^3\max\{1,K^4p_g/n_g\}},\frac{1}{G^2\max\{1,K^4p_0/n\}} \}
%	\]
%	%Let $r_g=Gn_g/n$.
%	Substituting these and setting $C=G^2$ we conclude with \eqref{learning rate rule} and we obtain the worst-case convergence rate (over all $\ddelta_i$ components) of
%	%\[
%	%\rho=1-\frac{\kappa^2}{8} \min\{\frac{Cn_g^4}{n^4G^3\Lambda_g},\frac{1}{G^2\Lambda_0} \}=1-\frac{\kappa^2}{8} \min\{\frac{Cr_g^4}{G^7\Lambda_g},\frac{1}{G^2\Lambda_0} \}.
%	%\]
%	%
%	%and implies the rate of convergence of
%	\[
%	\rho=1-\kappa \mu \min_{1\leq g\leq G}\{\frac{n_g^2}{n^2},C^{-1} \}= 1-\kappa\mu\min_{1\leq g\leq G}\frac{n_g^2}{n^2},
%	%\rho=1-C^{-1}\kappa \mu \min_{1\leq g\leq G}\{\frac{Cn_g^2}{n^2},1 \}= 1-c\frac{\kappa^2}{G^2} \min_{1\leq g\leq G}r_g^2 \min_{1\leq g\leq G}\{\frac{1}{G\max\{1,K^4p_g/n_g\}},\frac{1}{\max\{1,K^4p_0/n\}} \}
%	\]
%	where we used $\min_{1\leq g\leq G}\{\frac{n_g^2}{n^2}\}\leq 1/G^2$. This concludes the proof.
%	%and observing $\rho=1-\kappa\mu/G^2$
%	
%	%Similarly, applying standard eigenvalue bounds $\|\X_g^T\X_g\|\leq \Lambda_g=(\sqrt{n_g}+CK^2\sqrt{p_g})^2$ with probability $1-\exp(-p_g)$.
%\end{proof}
%%Checking $\ell_2$ vs $\ell_2^2$ condition: (using Cauchy-Schwarz)
%%\[
%%(G+1)\sum_{g=0}^G\frac{n_g^2}{n^2}\tn{\ddelta_g}^2\geq (\sum_{g=0}^G\frac{n_g}{n}\tn{\ddelta_g})^2\implies \left(\sqrt{\sum_{g=0}^G\frac{n_g^2}{n^2}\tn{\ddelta_g}^2}=\sqrt{1/(G+1)}\implies \sum_{g=0}^G\frac{n_g}{n}\tn{\ddelta_g}\geq 1\right)
%%\]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Samet Part Ends %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Cost of projection}
%{\color{red} TODO: Explain different methods to compute projection onto different norm balls.} 